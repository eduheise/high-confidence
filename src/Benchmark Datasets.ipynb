{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYN_DATASET_SIZE = 1000000\n",
    "\n",
    "BATCH_SIZE = 0.3\n",
    "EVAL_SIZE = 0.7\n",
    "\n",
    "N_BATCH = int(SYN_DATASET_SIZE * BATCH_SIZE)\n",
    "N_EVAL = int(SYN_DATASET_SIZE * EVAL_SIZE)\n",
    "\n",
    "CD_POSITION = N_BATCH + int(N_EVAL / 2)\n",
    "CD_WIDTH = int(N_EVAL * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML\n",
    "\n",
    "def train_model(X, y, model_name):\n",
    "    model_path = f'../output/models/{model_name}'\n",
    "\n",
    "    print(f'Dataset size: {X.shape}, {y.shape}')\n",
    "\n",
    "    automl = AutoML(results_path=model_path)\n",
    "    automl.fit(X, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(X, y, model_name, model_type):\n",
    "    model_path = f'../output/models/{model_name}'\n",
    "    output_file = f'../data/{model_type}/{model_name.lower()}.feather'\n",
    "\n",
    "    automl = AutoML(results_path=model_path)\n",
    "    prob_0, prob_1 = zip(*automl.predict_proba(X))\n",
    "    output_df = pd.DataFrame({'prob_negative': prob_0, 'label_negative': (y == 0).astype(int), 'prob_positive': prob_1, 'label_positive': y})\n",
    "    output_df = output_df.reset_index().rename(columns={'index': 'timestamp'})\n",
    "    output_df.to_feather(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ml3f-n9uWyTkLzRhtuEvDTBUl9SntFzY\n",
      "To: /home/eferrj/work/high-confidence/data/custom/application_01.feather\n",
      "100%|██████████| 2.87M/2.87M [00:00<00:00, 7.79MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FMk7VyjG4SRfS2wTIWle5WEEFRXqAf7J\n",
      "To: /home/eferrj/work/high-confidence/data/custom/application_02.feather\n",
      "100%|██████████| 50.3M/50.3M [00:08<00:00, 5.63MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1gK7HtaB4AMKe1_Iv-vOmDu9Rq5aaY16J\n",
      "To: /home/eferrj/work/high-confidence/data/custom/application_03.feather\n",
      "100%|██████████| 6.03M/6.03M [00:00<00:00, 9.67MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1TmgRmw4C5gr982RwbY-6v5SzVFNXmhH9\n",
      "To: /home/eferrj/work/high-confidence/data/custom/application_04.feather\n",
      "100%|██████████| 17.9M/17.9M [00:02<00:00, 6.36MB/s]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "apps = {'application_01': 'https://drive.google.com/uc?id=1ml3f-n9uWyTkLzRhtuEvDTBUl9SntFzY', \n",
    "        'application_02': 'https://drive.google.com/uc?id=1FMk7VyjG4SRfS2wTIWle5WEEFRXqAf7J', \n",
    "        'application_03': 'https://drive.google.com/uc?id=1gK7HtaB4AMKe1_Iv-vOmDu9Rq5aaY16J', \n",
    "        'application_04': 'https://drive.google.com/uc?id=1TmgRmw4C5gr982RwbY-6v5SzVFNXmhH9'}\n",
    "\n",
    "for app, url in apps.items():\n",
    "    gdown.download(url, f'../data/custom/{app}.feather', quiet=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Synthetic Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEA Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (300000, 3), (300000,)\n",
      "Linear algorithm was disabled.\n",
      "AutoML directory: ../output/models/SEA_V0_V1\n",
      "The task is binary_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Baseline', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 2 models\n",
      "1_Baseline logloss 0.62696 trained in 0.61 seconds\n",
      "2_DecisionTree logloss 0.181799 trained in 24.37 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "3_Default_Xgboost logloss 0.003343 trained in 34.37 seconds\n",
      "4_Default_NeuralNetwork logloss 0.001651 trained in 68.2 seconds\n",
      "5_Default_RandomForest logloss 0.085254 trained in 13.03 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 0.001651 trained in 1.3 seconds\n",
      "AutoML fit time: 149.81 seconds\n",
      "AutoML best model: 4_Default_NeuralNetwork\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but StandardScaler was fitted without feature names\n"
     ]
    }
   ],
   "source": [
    "from river import synth\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "gen = synth.ConceptDriftStream(stream=synth.SEA(seed=23, variant=0),\n",
    "                               drift_stream=synth.SEA(seed=23, variant=1),\n",
    "                               seed=23, position=CD_POSITION, width=CD_WIDTH)\n",
    "                               \n",
    "dataset = gen.take(SYN_DATASET_SIZE)\n",
    "\n",
    "data_dict = {'y': []}\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "df_train, df_eval = df.iloc[:N_BATCH, :], df.iloc[N_BATCH:, :]\n",
    "X_train, y_train = df_train[df_train.columns[1:]], df_train[df_train.columns[0]]\n",
    "X_eval, y_eval = df_eval[df_eval.columns[1:]], df_eval[df_eval.columns[0]]\n",
    "\n",
    "train_model(X_train, y_train, 'SEA_V0_V1')\n",
    "evaluate_model(X_eval, y_eval, 'SEA_V0_V1', 'synthetic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (300000, 4), (300000,)\n",
      "Linear algorithm was disabled.\n",
      "AutoML directory: ../output/models/mixed_recurrent\n",
      "The task is binary_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Baseline', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 2 models\n",
      "1_Baseline logloss 0.691259 trained in 0.83 seconds\n",
      "Exception while producing SHAP explanations. Passing parameters norm and vmin/vmax simultaneously is not supported. Please pass vmin/vmax directly to the norm when creating it.\n",
      "Continuing ...\n",
      "2_DecisionTree logloss 0.228663 trained in 19.65 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "Exception while producing SHAP explanations. Passing parameters norm and vmin/vmax simultaneously is not supported. Please pass vmin/vmax directly to the norm when creating it.\n",
      "Continuing ...\n",
      "3_Default_Xgboost logloss 0.003843 trained in 25.49 seconds\n",
      "4_Default_NeuralNetwork logloss 0.004813 trained in 121.13 seconds\n",
      "Exception while producing SHAP explanations. Passing parameters norm and vmin/vmax simultaneously is not supported. Please pass vmin/vmax directly to the norm when creating it.\n",
      "Continuing ...\n",
      "5_Default_RandomForest logloss 0.178896 trained in 11.54 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 0.003634 trained in 1.29 seconds\n",
      "AutoML fit time: 189.75 seconds\n",
      "AutoML best model: Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but StandardScaler was fitted without feature names\n"
     ]
    }
   ],
   "source": [
    "from river import synth\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "gen_01 = synth.Mixed(seed=23, classification_function=0)\n",
    "gen_02 = synth.Mixed(seed=23, classification_function=1)\n",
    "\n",
    "CONCEPT_INTERVAL = 100000\n",
    "\n",
    "data_dict = {'y': []}\n",
    "\n",
    "dataset = gen_01.take(N_BATCH)\n",
    "\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "dataset = gen_01.take(CONCEPT_INTERVAL*2)\n",
    "\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "\n",
    "dataset = gen_02.take(CONCEPT_INTERVAL)\n",
    "\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "dataset = gen_01.take(CONCEPT_INTERVAL)\n",
    "\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "dataset = gen_02.take(CONCEPT_INTERVAL)\n",
    "\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "\n",
    "dataset = gen_01.take(CONCEPT_INTERVAL)\n",
    "\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "df_train, df_eval = df.iloc[:N_BATCH, :], df.iloc[N_BATCH:, :]\n",
    "X_train, y_train = df_train[df_train.columns[1:]], df_train[df_train.columns[0]]\n",
    "X_eval, y_eval = df_eval[df_eval.columns[1:]], df_eval[df_eval.columns[0]]\n",
    "\n",
    "train_model(X_train, y_train, 'mixed_recurrent')\n",
    "evaluate_model(X_eval, y_eval, 'mixed_recurrent', 'synthetic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Real Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://maxhalford.github.io/files/datasets/electricity.zip (697.72 KB)\n",
      "Uncompressing into /home/eferrj/river_data/Elec2\n",
      "Dataset size: (13593, 8), (13593,)\n",
      "Linear algorithm was disabled.\n",
      "AutoML directory: ../output/models/elec2\n",
      "The task is binary_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Baseline', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 2 models\n",
      "1_Baseline logloss 0.686983 trained in 0.26 seconds\n",
      "2_DecisionTree logloss 0.375937 trained in 3.57 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "3_Default_Xgboost logloss 0.116597 trained in 12.7 seconds\n",
      "4_Default_NeuralNetwork logloss 0.352825 trained in 4.81 seconds\n",
      "5_Default_RandomForest logloss 0.347824 trained in 2.59 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 0.116597 trained in 0.8 seconds\n",
      "AutoML fit time: 32.93 seconds\n",
      "AutoML best model: 3_Default_Xgboost\n"
     ]
    }
   ],
   "source": [
    "from river.datasets import Elec2\n",
    "import pandas as pd\n",
    "\n",
    "elec2 = Elec2()\n",
    "if not elec2.is_downloaded:\n",
    "    elec2.download()\n",
    "\n",
    "dataset = elec2.take(45_312)\n",
    "\n",
    "data_dict = {'y': []}\n",
    "for X, y in dataset:\n",
    "    features = [x for x in X.keys()]\n",
    "    for feature in features:\n",
    "        if not feature in data_dict:\n",
    "            data_dict[feature] = []\n",
    "        data_dict[feature].append(X[feature])\n",
    "    data_dict['y'].append(int(y))\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "N_BATCH = int(df.shape[0] * 0.3)\n",
    "\n",
    "df_train, df_eval = df.iloc[:N_BATCH, :], df.iloc[N_BATCH:, :]\n",
    "X_train, y_train = df_train[df_train.columns[1:]], df_train[df_train.columns[0]]\n",
    "X_eval, y_eval = df_eval[df_eval.columns[1:]], df_eval[df_eval.columns[0]]\n",
    "\n",
    "train_model(X_train, y_train, 'elec2')\n",
    "evaluate_model(X_eval, y_eval, 'elec2', 'real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/scikit-multiflow/streaming-datasets/master/airlines.csv'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "with open('../datasets/airlines.csv', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (161814, 7), (161814,)\n",
      "Linear algorithm was disabled.\n",
      "AutoML directory: ../output/models/airlines\n",
      "The task is binary_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Baseline', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 2 models\n",
      "1_Baseline logloss 0.640783 trained in 0.45 seconds\n",
      "2_DecisionTree logloss 0.603446 trained in 17.45 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "Exception while producing SHAP explanations. Passing parameters norm and vmin/vmax simultaneously is not supported. Please pass vmin/vmax directly to the norm when creating it.\n",
      "Continuing ...\n",
      "3_Default_Xgboost logloss 0.55983 trained in 22.01 seconds\n",
      "4_Default_NeuralNetwork logloss 0.602219 trained in 71.06 seconds\n",
      "5_Default_RandomForest logloss 0.596631 trained in 14.48 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 0.55983 trained in 5.13 seconds\n",
      "AutoML fit time: 140.03 seconds\n",
      "AutoML best model: 3_Default_Xgboost\n"
     ]
    }
   ],
   "source": [
    "from river.datasets import AirlinePassengers\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('../datasets/airlines.csv')\n",
    "df = df.rename(columns={'Delay': 'y'})\n",
    "\n",
    "N_BATCH = int(df.shape[0] * 0.3)\n",
    "\n",
    "df_train, df_eval = df.iloc[:N_BATCH, :], df.iloc[N_BATCH:, :]\n",
    "X_train, y_train = df_train[[c for c in df_train.columns if c != 'y']], df_train['y']\n",
    "X_eval, y_eval = df_eval[[c for c in df_eval.columns if c != 'y']], df_eval['y']\n",
    "\n",
    "train_model(X_train, y_train, 'airlines')\n",
    "evaluate_model(X_eval, y_eval, 'airlines', 'real')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15e9e0b1061f5eda7a4215cf3d8c61dfd8baaceb935ab9f1d0ec67a0101dd5ad"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
